{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/jpeganovit/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import dct_manip as dm\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import utils.custom_transforms as ctrans\n",
    "from utils.pipeline_utils import load_model_and_report\n",
    "import utils.configs as configs\n",
    "import argparse\n",
    "from PIL import Image\n",
    "os.chdir(\"/workspace/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = \"dataset/mvtec_ad_jpeg/bottle/train/good/000.JPEG\"\n",
    "png_path = \"dataset/mvtec_ad/bottle/train/good/000.PNG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import MvtecAd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"dataset/mvtec_ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_dataset = MvtecAd(datadir = datapath,\n",
    "                        target = \"bottle\",\n",
    "                        is_train = True,\n",
    "                        resize = 224,\n",
    "                        image_mode = 'dct',\n",
    "                        image_format = 'jpeg',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute '노멛'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m jpeg_dataset[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m노멛\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '노멛'"
     ]
    }
   ],
   "source": [
    "jpeg_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim, quant, Y, cbcr = dm.read_coefficients(imgpath) # dimension, quantization, Y, cbcr coefficients\n",
    "Y = torch.clamp(Y * quant[0], min=-2**10, max=2**10-8) # recover quantized coefficients (clamp to -1024~1023 (values should be within -1024~1016 with quant table of all ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cbcr is not None: # if colored\n",
    "    cbcr = torch.clamp(cbcr * quant[1:3].unsqueeze(1).unsqueeze(1), min=-2**10, max=2**10-8) # recover quantized coefficient\n",
    "else: # if black and white\n",
    "    _, h, w, kh, kw = Y.shape\n",
    "    cbcr = torch.zeros((2,h//2,w//2,kh,kw), dtype=Y.dtype, device=Y.device) # fill it with zeroes\n",
    "coeffs = (Y, cbcr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                ctrans.Resize_DCT(28), # 28 = 224x224, 48 = 384x384\n",
    "                                ctrans.ToRange(val_min=-1, val_max=1, orig_min=-1024, orig_max=1016, dtype=torch.float32),\n",
    "                                ])\n",
    "coeffs = transform(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28, 8, 8]), torch.Size([2, 14, 14, 8, 8]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs[0].shape, coeffs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_patch_embedd = PatchEmbedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_patch_embedd(torch.randn(1,3,224,224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.decoder import *\n",
    "from models.plainvit import *\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchembed = PatchEmbedding_DCT_Group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 196, 768]), torch.Size([1, 14, 14, 384]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchembed(batch_coeffs)[0].shape, patchembed(batch_coeffs)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_coeffs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m temp_y \u001b[39m=\u001b[39m batch_coeffs[\u001b[39m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m rearranged_y \u001b[39m=\u001b[39m patchembed\u001b[39m.\u001b[39mrearrange_Y(temp_y)\n\u001b[1;32m      3\u001b[0m subblocked_y \u001b[39m=\u001b[39m apply_subblock(rearranged_y, patchembed\u001b[39m.\u001b[39mconv_Y, combine \u001b[39m=\u001b[39m patchembed\u001b[39m.\u001b[39mcombine_Y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_coeffs' is not defined"
     ]
    }
   ],
   "source": [
    "temp_y = batch_coeffs[0]\n",
    "rearranged_y = patchembed.rearrange_Y(temp_y)\n",
    "subblocked_y = apply_subblock(rearranged_y, patchembed.conv_Y, combine = patchembed.combine_Y)\n",
    "print(\"temp_y.shape\", temp_y.shape)\n",
    "print(\"rearranged_y.shape\", rearranged_y.shape)\n",
    "print(\"subblocked_y.shape\", subblocked_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_cbcr.shape torch.Size([1, 1, 28, 28, 8, 8])\n",
      "rearranged_cbcr.shape torch.Size([1, 2, 14, 14, 8, 8])\n",
      "subblocked_cbcr.shape torch.Size([1, 2, 14, 14, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "temp_cbcr = batch_coeffs[1]\n",
    "rearranged_cbcr = patchembed.rearrange_C(temp_cbcr)\n",
    "subblocked_cbcr = apply_subblock(rearranged_cbcr, patchembed.conv_C, combine = patchembed.combine_C)\n",
    "print(\"temp_cbcr.shape\", temp_y.shape)\n",
    "print(\"rearranged_cbcr.shape\", rearranged_cbcr.shape)\n",
    "print(\"subblocked_cbcr.shape\", subblocked_cbcr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14, 14, 256]), torch.Size([1, 14, 14, 128]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchembed.collapser(subblocked_y).shape, patchembed.collapser(subblocked_cbcr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 14, 384])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_ycbcr = torch.cat((patchembed.collapser(subblocked_y), patchembed.collapser(subblocked_cbcr)), dim=3)\n",
    "temp_ycbcr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_output = patchembed.projection(temp_ycbcr)\n",
    "project_output.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse arguments\n",
    "    \"\"\"\n",
    "    parser=argparse.ArgumentParser()\n",
    "\n",
    "    # DDP config\n",
    "    parser.add_argument('--port', type=int, default=13932, help='Port for pytorch distributed dataparallel')\n",
    "\n",
    "    # model config\n",
    "    parser.add_argument('--model_arch', type=str, default='vits', help='Model architecture (vitti, vits, vitb, vitl, swinv2)')\n",
    "    parser.add_argument('--no_subblock', action='store_true', help='If set, disable subblock conversion')\n",
    "    parser.add_argument(\"--embed_type\", type=int, default=1, help='Embedding layer type. (1: grouped, 2: separate, 3: concatenate). Default 1')\n",
    "    parser.add_argument(\"--domain\", type=str, default=\"dct\", help=\"(DCT/RGB) Choose domain type\")\n",
    "\n",
    "    # data config\n",
    "    parser.add_argument(\"--datapath\", type=str, default='./dataset/', help='Path to folder containing the .tar files')\n",
    "    parser.add_argument(\"--image_size\", type=int, default=224, help='Image size for training')\n",
    "    parser.add_argument(\"--object\", type=str, default='bottle', help='Object to train on')\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1, help='number of GPUs to use. If not set, automatically use all available GPUs')\n",
    "    parser.add_argument('--num_cpus', type=int, default=1, help='number of total available cpu threads')\n",
    "\n",
    "    # pipeline config\n",
    "    parser.add_argument('--train', action='store_true', help='Train new model')\n",
    "    parser.add_argument('--eval', action='store_true', help='Evaluate model loaded from ``savepath`` ')\n",
    "    parser.add_argument('--benchmark', type=int, default=0, help='If set, benchmark for the set iterations')\n",
    "    parser.add_argument('--savepath', type=str, default='./weights/jpeganovit.pth', help='Save path for model. Also saves checkpoint at this path')\n",
    "    parser.add_argument('--loadpath', type=str, default='', help='Load path for model. Used during evaluation. If empty, copy savepath')\n",
    "    parser.add_argument('--load_ckpt', type=str, default='', help='If set, load checkpoint from this path')\n",
    "    parser.add_argument('--deterministic', action='store_true', help='If set, use deterministic mode')\n",
    "    parser.add_argument('--verbose', type=int, default=0, help='(0/1/2) 0: no output, 1: output per epoch, 2: output per iteration')\n",
    "\n",
    "    # override default config\n",
    "    parser.add_argument(\"--epochs\", type=int, default=-1, help=\"Override the number of epochs\")\n",
    "    parser.add_argument(\"--batch\", type=int, default=-1, help=\"Override the size of batch (overall batch size)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=-1, help='Override the learning rate')\n",
    "    parser.add_argument(\"--wd\", type=float, default=-1, help='Override the weight decay strength')\n",
    "    parser.add_argument('--drop', type=float, default=-1, help='Override dropout probability')\n",
    "    parser.add_argument('--warmup_steps', type=int, default=-1, help='Override warmup steps')\n",
    "    parser.add_argument('--ops_list', type=str, default='', help='Override augmentation list')\n",
    "    parser.add_argument('--num_ops', type=int, default=-1, help='Override number of operations')\n",
    "    parser.add_argument('--ops_magnitude', type=int, default=-1, help='Override augmentation magnitude')\n",
    "    parser.add_argument(\"--amp\", type=int, default=-1, help=\"(True:1/False:0) Override automatic mixed precision\")\n",
    "    parser.add_argument(\"--ampdtype\", type=str, default='', help=\"Override amp dtype casting\")\n",
    "    parser.add_argument('--seed', type=int, default=-1, help='Override random seed')\n",
    "    parser.add_argument('--use_msrsync', action='store_true', help='If set, use msrsync instead of .tar')\n",
    "\n",
    "    args=parser.parse_args(args=[])  \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "cfg = configs.generate_config(\n",
    "        modelarch = args.model_arch.lower(),\n",
    "        domain = args.domain,\n",
    "        modelver=args.embed_type,\n",
    "        subblock=True if not args.no_subblock else False,\n",
    "        epochs=None if args.epochs < 0 else args.epochs, # need to add\n",
    "        batchsize=None if args.batch < 0 else args.batch, # need to change order\n",
    "        lr=None if args.lr < 0 else args.lr,\n",
    "        wd=None if args.wd < 0 else args.wd,\n",
    "        drop=None if args.drop < 0 else args.drop,\n",
    "        warmup_steps=None if args.warmup_steps < 0 else args.warmup_steps, # need to add\n",
    "        auglist=None if args.ops_list == '' else args.ops_list.split(\",\"),\n",
    "        num_ops=None if args.num_ops < 0 else args.num_ops, # need to add\n",
    "        ops_magnitude=None if args.ops_magnitude < 0 else args.ops_magnitude, # need to add\n",
    "        seed=None if args.seed < 0 else args.seed, # need to add\n",
    "        amp=None if args.amp < 0 else args.amp,\n",
    "        ampdtype=None if args.ampdtype == '' else args.ampdtype,\n",
    "        use_msrsync=args.use_msrsync,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitmodel = ViT(\n",
    "                in_channels= 3,\n",
    "                patch_size= cfg.MODEL.PATCHSIZE,\n",
    "                emb_size= cfg.MODEL.EMBEDSIZE,\n",
    "                depth= cfg.MODEL.DEPTH,\n",
    "                n_classes= 1000,\n",
    "                drop_p=cfg.TRAIN.DROP,\n",
    "                device=cfg.RANK,\n",
    "                dtype=torch.float32,\n",
    "                num_heads=cfg.MODEL.HEADS,\n",
    "                head_size=cfg.MODEL.HEADSIZE,\n",
    "                pixel_space=cfg.MODEL.DOMAIN,\n",
    "                ver=cfg.MODEL.VERSION,\n",
    "                use_subblock=cfg.MODEL.SUBBLOCK,\n",
    "                )\n",
    "decoder = Decoder(\n",
    "                image_size=args.image_size,\n",
    "                patch_size=cfg.MODEL.PATCHSIZE,\n",
    "                emb_size=cfg.MODEL.EMBEDSIZE,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change it to batch size 1\n",
    "batch_coeffs = (coeffs[0].unsqueeze(0), coeffs[1].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_location = {'cuda:%d' % 0: 'cuda:%d' % cfg.RANK}\n",
    "vitmodel = vitmodel.to(cfg.RANK)\n",
    "vitmodel_state_dict = torch.load(\"weights/imgnetDCTViTS_ep90_76.5.pth\", map_location=map_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitmodel.load_state_dict(vitmodel_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 384])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitmodel.patchembed(batch_coeffs[0].to(\"cuda:0\"), batch_coeffs[1].to(\"cuda:0\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_output = vitmodel(batch_coeffs[0].to(\"cuda:0\"), batch_coeffs[1].to(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ViT(\n",
       "    (patchembed): PatchEmbedding_DCT_Group(\n",
       "      (rearrange_Y): Rearrange('b c (h pdh) (w pdw) p1 p2 -> b c h w (pdh p1) (pdw p2)', pdh=2, pdw=2)\n",
       "      (rearrange_C): Rearrange('b c (h pdh) (w pdw) p1 p2 -> b c h w (pdh p1) (pdw p2)', pdh=1, pdw=1)\n",
       "      (collapser): Rearrange('b c h w i j -> b h w (c i j)')\n",
       "      (projection): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (1): SinCosEmbedding()\n",
       "        (2): Rearrange('b h w e -> b (h w) e')\n",
       "      )\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (0): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): TransformerEncoderBlock(\n",
       "        (0): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_mha): MultiHeadAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (att_drop): Dropout(p=0, inplace=False)\n",
       "              (projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualAdd(\n",
       "          (fn): Sequential(\n",
       "            (eb_lrnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (eb_ffb): FeedForwardBlock(\n",
       "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (1): GELU(approximate=none)\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            )\n",
       "            (eb_drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classhead): ClassificationHead(\n",
       "      (ch_lrnorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ch_gap): Reduce('b n e -> b e', 'mean')\n",
       "      (ch_linear1): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (ch_tanh): Tanh()\n",
       "      (ch_linear2): Linear(in_features=384, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): Decoder(\n",
       "    (dec_block1): Sequential(\n",
       "      (0): ConvTranspose2d(384, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (dec_block2): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (dec_block3): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (dec_block4): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (dec_block5): Sequential(\n",
       "      (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (up): UpsamplingBilinear2d(size=(224, 224), mode=bilinear)\n",
       "    (output): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model = nn.Sequential(vitmodel, decoder)\n",
    "full_model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_batch_coeffs = (batch_coeffs[0].to(\"cuda:0\"), batch_coeffs[1].to(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/jpeganovit/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.forward(cuda_batch_coeffs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 28, 28, 8, 8]), torch.Size([1, 2, 14, 14, 8, 8]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_coeffs[0].shape, batch_coeffs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384, 196])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_output = vit_output.transpose(1,2)\n",
    "transpose_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384, 14, 14])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape_output = transpose_output.reshape(vit_output.shape[0], -1, 14, 14)\n",
    "reshape_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 16, 16])\n",
      "torch.Size([1, 128, 18, 18])\n",
      "torch.Size([1, 64, 20, 20])\n",
      "torch.Size([1, 32, 39, 39])\n",
      "torch.Size([1, 16, 77, 77])\n"
     ]
    }
   ],
   "source": [
    "dec_block1_output = decoder.dec_block1(reshape_output)\n",
    "dec_block2_output = decoder.dec_block2(dec_block1_output)\n",
    "dec_block3_output = decoder.dec_block3(dec_block2_output)\n",
    "dec_block4_output = decoder.dec_block4(dec_block3_output)\n",
    "dec_block5_output = decoder.dec_block5(dec_block4_output)\n",
    "print(dec_block1_output.shape)\n",
    "print(dec_block2_output.shape)\n",
    "print(dec_block3_output.shape)\n",
    "print(dec_block4_output.shape)\n",
    "print(dec_block5_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 224, 224])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up_output = decoder.up(dec_block5_output)\n",
    "up_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsamplingBilinear2d(size=(224, 224), mode=bilinear)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_output = decoder.output(up_output)\n",
    "output_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_decoder_up = 1, 14, 14, 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_transforms import ycbcr_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28, 8, 8])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycbcr_rgb = ycbcr_to_rgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ycbcr_rgb(coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# can be reshaped to something suitable for transposed convolution.\n",
    "DeconvY = nn.ModuleList([nn.ConvTranspose2d(64, 64, kernel_size=(8, 8)) for _ in range(8)])\n",
    "DeconvC = nn.ModuleList([nn.ConvTranspose2d(64, 64, kernel_size=(8, 8)) for _ in range(8)])\n",
    "\n",
    "def backward(ycbcr, patch_embbed_size: int= 28):\n",
    "    # Reshape the input to the shape before the projection\n",
    "    b, _, e = ycbcr.shape\n",
    "    y_size, cbcr_size = patch_embbed_size, patch_embbed_size/2\n",
    "    h = int((b / e) ** 0.5)\n",
    "    w = h\n",
    "    ycbcr = Rearrange(ycbcr, \"b (h w) e -> b h w e\", h=h, w=w)\n",
    "\n",
    "    # Determine the original channel counts for Y and CbCr\n",
    "    y_shape, cbcr_shape = (1, y_size, y_size, 8, 8), (2, cbcr_size, cbcr_size, 8, 8)\n",
    "    _, _, _, cy = y_shape\n",
    "    _, _, _, cc = cbcr_shape\n",
    "\n",
    "    yout = ycbcr[:, :, :, :cy]\n",
    "    cout = ycbcr[:, :, :, cy:cy+cc]\n",
    "\n",
    "    # Upsample the Y channel using DeconvY\n",
    "    y = []\n",
    "    for i, deconv in enumerate(DeconvY):\n",
    "        y.append(deconv(yout[:, :, :, i]))\n",
    "    y = torch.cat(y, dim=1)\n",
    "\n",
    "    # Upsample the CbCr channel using DeconvC\n",
    "    cbcr = []\n",
    "    for i, deconv in enumerate(DeconvC):\n",
    "        cbcr.append(deconv(cout[:, :, :, i]))\n",
    "    cbcr = torch.cat(cbcr, dim=1)\n",
    "\n",
    "    # Assuming the output size from deconvolution isn't the same as the desired\n",
    "    # (1, 28, 28, 8, 8) and (2, 14, 14, 8, 8), we will reshape them.\n",
    "    y = y.view(b, 1, 28, 28, 8, 8)\n",
    "    cbcr = cbcr.view(b, 2, 14, 14, 8, 8)\n",
    "\n",
    "    return y, cbcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycbcr_patch = vitmodel.patchembed(batch_coeffs[0].to(\"cuda:0\"), batch_coeffs[1].to(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 384])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ycbcr_patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28, 8, 8])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_coeffs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 14, 14, 16, 16])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitmodel.patchembed.rearrange_Y(batch_coeffs[0].to(\"cuda:0\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_transforms import ycbcr_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ycbcr_to_rgb.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ycbcr_to_rgb(coeffs)\n",
      "\u001b[0;31mTypeError\u001b[0m: ycbcr_to_rgb.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "ycbcr_to_rgb(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempa, tempb = coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28, 8, 8]), torch.Size([2, 14, 14, 8, 8]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempa.shape, tempb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpeganovit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
