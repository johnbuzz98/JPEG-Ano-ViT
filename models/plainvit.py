###################################################################################################
###
### This is a modified version of a code by Francesco Zuppichini (Jan 1, 2021) 
### (link: https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)
###
###################################################################################################

from collections import OrderedDict

import torch
import torch.nn.functional as F
from einops import rearrange
from einops.layers.torch import Rearrange, Reduce
from torch import Tensor, nn

import utils.dct_ops as dops


def patch2subblock(patch_size, generate_matrix=True, dtype=torch.float32, device='cpu'):
    """
    Generate appropriate subblock conversion matrix for given patch size
    Inputs:
        patch_size (must be power of 2 and >= 2)
        generate_matrix: If true, generate conversion matrix. If not, only calculate patch_dim
        dtype
        device

    Outputs:
        Conversion matrix (can be None if conversion is not needed)
        patch_dim: (if patch_size > 8): how many 8x8 blocks need to be grouped together to form patch_size (per one side, H or W)
                   (if patch_size < 8): how many patches need to be grouped together to form 8x8
    """
    if patch_size > 8: # need to combine
        patch_dim = patch_size // 8
        if generate_matrix:
            convmat = dops.generate_conversion_matrix(length_small=8, mult=patch_dim, scale=True, dtype=dtype, device=device)
        else:
            convmat = None
    elif patch_size == 8: # no need for any conversion
        patch_dim = 1
        convmat = None
    elif patch_size < 8: # need to decompose
        patch_dim = 8 // patch_size
        if generate_matrix:
            convmat = dops.generate_conversion_matrix(length_small=patch_size, mult=patch_dim, scale=True, dtype=dtype, device=device)
        else:
            convmat = None
    return convmat, patch_dim

def apply_subblock(coeff, convMat, combine=True):
    """
    Apply subblock conversion to input DCT coefficients.

    Inputs:
        coeff: DCT coefficient (B, C, H, W, KH, KW) or (B, H, W, C, KH, KW). KH, KW typically 8
        convMat: conversion matrix. If this is None, then return coeff without any conversion
        combine: If true, apply conversion such that it combines (KH, KW) blocks to a larger DCT bases
                 If false, apply conversion such that it decomposes (KH, KW) block into smaller DCT blocks 
    """
    if convMat == None:
        return coeff

    if combine: # combine blocks
        coeff = torch.einsum("i o, b c h w o j -> b c h w i j", convMat, coeff)
        coeff = torch.einsum("b c h w i o, o j -> b c h w i j", coeff, convMat.T)
    else: # decompose blocks (notice the difference in transpose)
        coeff = torch.einsum("i o, b c h w o j -> b c h w i j", convMat.T, coeff)
        coeff = torch.einsum("b c h w i o, o j -> b c h w i j", coeff, convMat)
    return coeff

def patch2rearrange(patch_size, patch_dim):
    """
    Generate appropriate rearrange layer for given patch size
    Inputs:
        patch_size: patch size for this channel (Y or CbCr)
        patch_dim: patch_dim generated by ``patch2subblock'' function
        
    Outputs:
        Rearrange layer (einops)
        Combine flag: (T/F) If true, subblock conversion combine blocks (if subblock conversion is used). If false, it instead decomposes a block
    """
    if patch_size >= 8:
        rearranger = Rearrange('b c (h pdh) (w pdw) p1 p2 -> b c h w (pdh p1) (pdw p2)', pdh=patch_dim, pdw=patch_dim) # each block gets larger
        combine = True # need to combine
    else:
        rearranger = Rearrange('b c h w (p1 pdh) (p2 pdw) -> b c (h pdh) (w pdw) p1 p2', pdh=patch_dim, pdw=patch_dim) # each block gets smaller
        combine = False # need to decompose
    return rearranger, combine

class SinCosEmbedding(nn.Module):
    """
    Sin-Cos positional embedding for 2-D input
    """
    def __init__(self):
        super().__init__()
    
    def forward(self, x: Tensor):
        #assert len(x.shape) == 4, "x should be 4 dimensions of b, h, w, e"
        if len(x.shape)==4:
            _, h, w, e = x.shape
        elif len(x.shape)==5:
            _, _, h, w, e = x.shape
        assert e % 4 == 0, "Embedding size should be multiple of 4"
        posemb_pos_h = torch.arange(h, device=x.device)
        posemb_pos_w = torch.arange(w, device=x.device)
        hgrid, wgrid = torch.meshgrid(posemb_pos_h, posemb_pos_w, indexing='ij')

        posemb_freq = torch.log(torch.tensor(10000, dtype=torch.int32, device=x.device)) / (e // 4 - 1)
        posemb_freq = torch.exp(-torch.arange(e // 4, dtype=x.dtype, device=x.device) * posemb_freq)
        
        posemb_posfreq_h = torch.einsum("p,f->pf", hgrid.flatten(), posemb_freq)
        posemb_posfreq_w = torch.einsum("p,f->pf", wgrid.flatten(), posemb_freq)
        posemb_comb = torch.cat(
                (posemb_posfreq_w.sin(), posemb_posfreq_w.cos(),
                posemb_posfreq_h.sin(), posemb_posfreq_h.cos()), dim=-1
            ).view((1, h, w, e))
        
        if len(x.shape)==5:
            posemb_comb = posemb_comb.unsqueeze(1)

        return x + posemb_comb # return summation

class PatchEmbedding(nn.Module):
    """
    Patch embedding layer (RGB)
    Use Conv2d to extract the input patches and convert them to embeddings.

    in_channels: input image channels
    patch_size: patch size for images (larger patches -> faster (not shown in #params))
    emb_size: embedding size
    img_size: size of input image
    class_mode: classification mode. 'token' or 'avgpool'
    position_mode: positional embedding mode. 'learn' or 'sincos'
    device: device to put the image to
    dtype: dtype of the model
    """
    def __init__(
            self, in_channels: int = 3, patch_size: int = 16, 
            emb_size: int = 768, 
            device='cpu', dtype=torch.float32,
        ):
        self.patch_size = patch_size
        super().__init__()
        self.projection = nn.Sequential(
            # using a conv layer instead of a linear one -> performance gains
            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size, device=device, dtype=dtype), # this part may need to change if I'm using blocks
            Rearrange('b e h w -> b h w e'),
            SinCosEmbedding(),
            Rearrange('b (h) (w) e -> b (h w) e'),
        )
        
    def forward(self, x: Tensor) -> Tensor:
        b, _, _, _ = x.shape
        x = self.projection(x)
        return x

class PatchEmbedding_DCT_Group(nn.Module):
    """
    Patch embedding layer for DCT
    
    Grouped embedding for YCbCr. Combine Y/CbCr blocks appropriately if `use_subblock' is True.

    patch_size: patch size for images (larger patches -> faster (not shown in #params))
    emb_size: embedding size per image channel (Y, Cb, Cr) # usually /3 the RGB embedding size
    use_subblock: If true, use subblock conversion
    chroma_scale: how small chroma dimension is compared to luma dimension. Default=2 for 4:2:0 chroma subsampling (2H, 2W -> H, W: chroma_scale=2)
    device: device to put the image to
    dtype: dtype of the model
    """
    def __init__(
            self, patch_size: int = 16, # Input: (B, nC, H, W, 8, 8)
            emb_size: int = 768, use_subblock=True,
            chroma_scale=2,
            device='cpu', dtype=torch.float32,
        ):
        super().__init__()
        assert not(patch_size & (patch_size - 1)) and patch_size != 0 and patch_size >= 2, f"Patch size should be 2^n (n>0, n=int). Current value: {patch_size}"
        self.patch_size = patch_size
        patch_size_C = patch_size // chroma_scale
        if patch_size < 8:
            assert use_subblock, f"When patch size is less than 8, subblock conversion is needed to decompose 8x8 blocks. Current patch size: {patch_size}, use_subblock: {use_subblock}"
        if patch_size_C < 8:
            assert use_subblock, f"When patch size goes below 8 (due to chroma subsampling of JPEG), subblock conversion is needed to decompose 8x8 blocks. Current patch size (CbCr): {patch_size_C}, patch_size (Y): {patch_size}, chroma_scale: {chroma_scale}, use_subblock: {use_subblock}"
        
        self.conv_Y, patch_dim_Y = patch2subblock(patch_size, generate_matrix=use_subblock, dtype=dtype, device=device)
        self.conv_C, patch_dim_C = patch2subblock(patch_size // chroma_scale, generate_matrix=use_subblock, dtype=dtype, device=device)
        linear_input_Y = patch_size ** 2
        linear_input_C = patch_size_C ** 2

        self.rearrange_Y, self.combine_Y = patch2rearrange(patch_size, patch_dim_Y)
        self.rearrange_C, self.combine_C = patch2rearrange(patch_size_C, patch_dim_C)
        self.collapser = Rearrange('b c h w i j -> b h w (c i j)') # collapse channel and patch dimensions

        self.projection = nn.Sequential( # project after concatenating Y and CbCr embeddings at (pdh, pdw) dim
            nn.Linear(linear_input_Y + linear_input_C*2, emb_size, device=device, dtype=dtype), # b c h w e
            SinCosEmbedding(),
            Rearrange('b h w e -> b (h w) e'),
        )

    def forward(self, *args) -> Tensor:
        # Handle the input based on the number of arguments
        if len(args) == 2:  # two separate arguments for y and cbcr
            y, cbcr = args[0][0], args[0][1]
        elif len(args) == 1 and isinstance(args[0], list) and len(args[0]) == 2:  # one tuple argument (y, cbcr)
            y, cbcr = args[0], args[1]
        else:
            raise ValueError("Invalid input. Expecting either y and cbcr separately or as a tuple (y, cbcr).")

        b, _, _, _, _, _ = y.shape
        if self.combine_Y:
            y = self.rearrange_Y(y)
            y = apply_subblock(y, self.conv_Y, combine=self.combine_Y)
        else:
            y = apply_subblock(y, self.conv_Y, combine=self.combine_Y)
            y = self.rearrange_Y(y) 
        if self.combine_C:
            cbcr = self.rearrange_C(cbcr) 
            cbcr = apply_subblock(cbcr, self.conv_C, combine=self.combine_C)
        else:
            cbcr = apply_subblock(cbcr, self.conv_C, combine=self.combine_C)
            cbcr = self.rearrange_C(cbcr) 
        y = self.collapser(y)
        cbcr = self.collapser(cbcr)
        ycbcr_0 = torch.cat([y, cbcr], dim=3) # b h w (pdY + pdC) (p1 p2)
        ycbcr_1 = self.projection(ycbcr_0) # b (h w) e; h/w of Y
        return ycbcr_1 #,ycbcr_0 #result and intermediate result before projection

class PatchEmbedding_DCT_Separate(nn.Module):
    """
    Patch embedding layer for DCT
    
    Separate embedding for ALL Y and CbCr blocks (Y_1, Y_2, Y_3, Y_4, Cb, Cr -> All generate size 64 embedding) + Concatenate

    patch_size: patch size for images (larger patches -> faster (not shown in #params))
    emb_size: embedding size per image channel (Y, Cb, Cr) # usually /3 the RGB embedding size    
    chroma_scale: how small chroma dimension is compared to luma dimension. Default=2 for 4:2:0 chroma subsampling (2H, 2W -> H, W: chroma_scale=2)
    device: device to put the image to
    dtype: dtype of the model
    """
    def __init__(
            self, patch_size: int = 16, # Input: (B, nC, H, W, 8, 8)
            emb_size: int = 768,
            chroma_scale=2,
            device='cpu', dtype=torch.float32,
        ):
        super().__init__()
        assert not(patch_size & (patch_size - 1)) and patch_size != 0 and patch_size >= 2, f"Patch size should be 2^n (n>0, n=int). Current value: {patch_size}"
        self.patch_size = patch_size
        assert patch_size // chroma_scale >= 8, f"Patch size for both Y and CbCr should be larger than 8 for this embedding method. Current patch size (Y): {patch_size}, chroma scale: {chroma_scale}, patch_size (CbCr): {patch_size // chroma_scale}"

        patch_dim_Y = patch_size // 8 # all DCT patches are 8x8 therefore this determines how many DCT patches need to be grouped together
        patch_dim_C = patch_dim_Y // chroma_scale # scale down chroma channels
        numBlockInPatch = patch_dim_Y**2 + (2*patch_dim_C**2) # number of DCT block in a patch if not using subblock

        self.LinearY = nn.ModuleList([nn.Linear(64, emb_size//numBlockInPatch, device=device, dtype=dtype) for _ in range(patch_dim_Y**2)]) # y
        self.LinearC = nn.ModuleList([nn.Linear(64, emb_size//numBlockInPatch, device=device, dtype=dtype) for _ in range(2*patch_dim_C**2)]) # cb, cr
        self.PreMixActivation = nn.GELU() # pre-channel mix activation function
        self.LinearMix = nn.Linear((emb_size//numBlockInPatch)*numBlockInPatch, emb_size, device=device, dtype=dtype) # embedding dim mixer (mix channel info)

        self.rearrange_Y = Rearrange('b c (h pdh) (w pdw) p1 p2 -> b h w (c pdh pdw) (p1 p2)', pdh=patch_dim_Y, pdw=patch_dim_Y)
        self.rearrange_C = Rearrange('b c (h pdh) (w pdw) p1 p2 -> b h w (c pdh pdw) (p1 p2)', pdh=patch_dim_C, pdw=patch_dim_C)
        self.projection = nn.Sequential(# project after concatenating Y and CbCr embeddings at (pdh, pdw) dim
            self.PreMixActivation,
            self.LinearMix,
            SinCosEmbedding(),
            Rearrange('b h w e -> b (h w) e'),
        )
        
    def forward(self, y, cbcr) -> Tensor:
        b, _, _, _, _, _ = y.shape
        y = self.rearrange_Y(y) # b h w (c pdh pdw) (p1 p2) where pdh, pdw = defualt 2, p1, p2 = default 8
        cbcr = self.rearrange_C(cbcr) # b h w (c pdh pdw) (p1 p2) where pdh, pdw = defualt 1; h, w = default h/2 w/2 vs Y; p1, p2 = default 8
        _, _, _, cy, _ = y.shape # number of channels
        _, _, _, cc, _ = cbcr.shape
        
        yout = self.LinearY[0](y[:,:,:,0:1,:])
        cout = self.LinearC[0](cbcr[:,:,:,0:1,:])
        for i in range(1, cy):
            yout = torch.cat([yout, self.LinearY[i](y[:,:,:,i:i+1,:])], dim=3)
        for i in range(1, cc):
            cout = torch.cat([cout, self.LinearC[i](cbcr[:,:,:,i:i+1,:])], dim=3)

        ycbcr = torch.cat([yout, cout], dim=3) # b h w (cy cc) 64
        ycbcr_0 = rearrange(ycbcr, "b h w c e -> b h w (c e)")
        ycbcr_1 = self.projection(ycbcr_0) # b (h w) e; h/w of Y (sincos embedding (if used))
        return ycbcr_1, #ycbcr_0 #result and intermediate result before projection

class PatchEmbedding_DCT_Separate_subblock(nn.Module):
    """
    Patch embedding layer for DCT
    
    Combine (2,2,8,8) blocks to (16,16) block + Separate embedding for Y and CbCr channels (always uses subblock)

    patch_size: patch size for images (larger patches -> faster (not shown in #params))
    emb_size: embedding size per image channel (Y, Cb, Cr) # usually /3 the RGB embedding size
    chroma_scale: how small chroma dimension is compared to luma dimension. Default=2 for 4:2:0 chroma subsampling (2H, 2W -> H, W: chroma_scale=2)
    device: device to put the image to
    dtype: dtype of the model
    """
    def __init__(
            self, patch_size: int = 16, # Input: (B, nC, H, W, 8, 8)
            emb_size: int = 768,
            chroma_scale=2,
            device='cpu', dtype=torch.float32,
        ):
        super().__init__()
        assert not(patch_size & (patch_size - 1)) and patch_size != 0 and patch_size >= 2, f"Patch size should be 2^n (n>0, n=int). Current value: {patch_size}"
        self.patch_size = patch_size
        patch_size_C = patch_size // chroma_scale
        
        self.conv_Y, patch_dim_Y = patch2subblock(patch_size, generate_matrix=True, dtype=dtype, device=device)
        self.conv_C, patch_dim_C = patch2subblock(patch_size // chroma_scale, generate_matrix=True, dtype=dtype, device=device)

        self.rearrange_Y, self.combine_Y = patch2rearrange(patch_size, patch_dim_Y)
        self.rearrange_C, self.combine_C = patch2rearrange(patch_size_C, patch_dim_C)

        linear_input_Y = (patch_dim_Y ** 2) * 8 * 8 # patch_dim ** 2 * (DCT_H * DCT_W) # 4 * 64
        linear_input_C = (patch_dim_C ** 2) * 8 * 8 # patch_dim ** 2 * (DCT_H * DCT_W) # 1 * 64

        self.projection_Y = nn.Sequential(
            Rearrange("b c h w i j -> b h w (c i j)"),
            nn.Linear(linear_input_Y, emb_size//6*4, device=device, dtype=dtype),
        )
        self.projection_C = nn.Sequential(
            Rearrange("b c h w i j -> b h w (c i j)"),
            nn.Linear(linear_input_C*2, emb_size//6*2, device=device, dtype=dtype),
        )
        self.gelu = nn.GELU() # GELU activation
        self.linearMix = nn.Linear(emb_size, emb_size, device=device, dtype=dtype) # b c h w e # mix channel info
        self.sincos = SinCosEmbedding()
        self.rearrange = Rearrange('b h w e -> b (h w) e')

        
    def forward(self, y, cbcr) -> Tensor:
        b, _, _, _, _, _ = y.shape
        if self.combine_Y:
            y = self.rearrange_Y(y)
            y = apply_subblock(y, self.conv_Y, combine=self.combine_Y)
        else:
            y = apply_subblock(y, self.conv_Y, combine=self.combine_Y)
            y = self.rearrange_Y(y) 
        if self.combine_C:
            cbcr = self.rearrange_C(cbcr) 
            cbcr = apply_subblock(cbcr, self.conv_C, combine=self.combine_C)
        else:
            cbcr = apply_subblock(cbcr, self.conv_C, combine=self.combine_C)
            cbcr = self.rearrange_C(cbcr) 

        y = self.projection_Y(y)
        cbcr = self.projection_C(cbcr)

        ycbcr = torch.cat([y, cbcr], dim=3) # b h w (cY cC)
        ycbcr = self.gelu(ycbcr) # mistakenly not added
        residual = ycbcr
        ycbcr = self.linearMix(ycbcr)
        ycbcr += residual # residual connection
        ycbcr = self.sincos(ycbcr)
        ycbcr = self.rearrange(ycbcr)
        return ycbcr

class PatchEmbedding_DCT_Concat(nn.Module):
    """
    Patch embedding layer for DCT

    Concatenate-based embedding. Separately embed Y and CbCr and then concatenate them

    patch_size: patch size for images (larger patches -> faster (not shown in #params))
    emb_size: embedding size per image channel (Y, Cb, Cr) # usually /3 the RGB embedding size
    use_subblock: If true, use subblock conversion
    img_size: size of input image
    device: device to put the image to
    dtype: dtype of the model
    """
    def __init__(
            self, patch_size: int = 16, # Input: (B, nC, H, W, 8, 8)
            emb_size: int = 768, use_subblock=True,
            device='cpu', dtype=torch.float32,
        ):
        super().__init__()
        assert not(patch_size & (patch_size - 1)) and patch_size != 0 and patch_size >= 2, f"Patch size should be 2^n (n>0, n=int). Current value: {patch_size}"
        if patch_size < 8:
            assert use_subblock, f"When patch size is less than 8, subblock conversion is needed to decompose 8x8 blocks. Current patch size: {patch_size}, use_subblock: {use_subblock}"

        self.patch_size = patch_size
        self.convMat, patch_dim = patch2subblock(patch_size, generate_matrix=use_subblock, dtype=dtype, device=device) # same patch size for both Y and CbCr
        linear_input = (patch_size ** 2) # same linear_input for Y and CbCr
        self.rearrange, self.combine = patch2rearrange(patch_size, patch_dim)

        self.projectionY = nn.Sequential(
            Rearrange('b c h w i j -> b c h w (i j)'),
            nn.Linear(linear_input, emb_size, device=device, dtype=dtype), # b c h w e
            SinCosEmbedding(),
            Rearrange('b c h w e -> b (c h w) e'),
        )
        self.projectionC = nn.Sequential(
            Rearrange('b c h w i j -> b c h w (i j)'),
            nn.Linear(linear_input, emb_size, device=device, dtype=dtype), # b c h w e
            SinCosEmbedding(),
            Rearrange('b c h w e -> b (c h w) e'),
        )

    def forward(self, y, cbcr) -> Tensor:
        b, _, _, _, _, _ = y.shape
        if self.combine: # combine subblock
            y = self.rearrange(y)
            cbcr = self.rearrange(cbcr)
            y = apply_subblock(y, self.convMat, combine=self.combine)
            cbcr = apply_subblock(cbcr, self.convMat, combine=self.combine) # b c h w kh kw
        else: # decompose subblock
            y = apply_subblock(y, self.convMat, combine=self.combine)
            cbcr = apply_subblock(cbcr, self.convMat, combine=self.combine) # b c h w kh kw
            y = self.rearrange(y)
            cbcr = self.rearrange(cbcr)

        y = self.projectionY(y) # b (hw) e
        cbcr = self.projectionC(cbcr) # b (2 * hw/4) e
        ycbcr = torch.cat([y, cbcr], dim=1) # b (hw * (1.5)) e
        return ycbcr

class MultiHeadAttention(nn.Module):
    """
    Multi-headed attention block
    1. Extracts queries, keys, and values from the embedding
    2. Then calculate the 'energy' (query * key) for each embeddings
    3. Scale the energy and use softmax to calculate attention
    4. Use attention & value to determine the output.

    # input_embed: default -1. If set, use different input embedding dimension
    """
    def __init__(
            self, emb_size: int = 768, input_embed: int = -1,
            num_heads: int = 8, head_size: int = 64, dropout: float = 0, nohidden=False,
            device='cpu', dtype=torch.float32, 
        ):
        super().__init__()
        self.emb_size = emb_size
        self.num_heads = num_heads
        self.nohidden = nohidden
        inner_size = num_heads * head_size # inner size per q, k, or v // 'd' in 'rearrange'
        if input_embed < 0: # if not set, default to embed size
            input_embed = emb_size

        # fuse the queries, keys and values in one matrix
        if nohidden:
            assert inner_size == emb_size, "num_heads*head_size should equal to emb_size if not using hidden layer."
            self.qkv = nn.Linear(input_embed, inner_size * 3, device=device, dtype=dtype)
            self.att_drop = nn.Dropout(dropout)
        else:
            self.qkv = nn.Linear(input_embed, inner_size * 3, device=device, dtype=dtype)
            self.att_drop = nn.Dropout(dropout)
            self.projection = nn.Linear(inner_size, emb_size, device=device, dtype=dtype)
        
    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:
        # split keys, queries and values in num_heads
        qkv = rearrange(self.qkv(x), "b n (h d qkv) -> (qkv) b h n d", h=self.num_heads, qkv=3) # d = head_size
        queries, keys, values = qkv[0], qkv[1], qkv[2]
        # sum up over the last axis (matmul last axis)
        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len
        if mask is not None:
            fill_value = torch.finfo(torch.float32).min # negative infinity
            energy.mask_fill(~mask, fill_value) # hide masked value as -inf
            
        scaling = self.emb_size ** (1/2)
        #att = F.softmax(energy, dim=-1) / scaling
        att = F.softmax(energy / scaling, dim=-1) # Corrected code.
        att = self.att_drop(att)
        # sum up over the third axis
        out = torch.einsum('bhal, bhlv -> bhav ', att, values) #matmul on last two dims
        out = rearrange(out, "b h n d -> b n (h d)") # sum over attention * values
        if not self.nohidden:
            out = self.projection(out)
        return out

    
class ResidualAdd(nn.Module):
    """
    Resblock
    """
    def __init__(self, fn):
        super().__init__()
        self.fn = fn
        
    def forward(self, x, **kwargs):
        res = x
        x = self.fn(x, **kwargs)
        x += res
        return x
    
class FeedForwardBlock(nn.Sequential):
    """
    Linear feedforward residual block
    """
    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0., device='cpu', dtype=torch.float32):
        super().__init__(
            nn.Linear(emb_size, expansion * emb_size, device=device, dtype=dtype),
            nn.GELU(),
            nn.Dropout(drop_p),
            nn.Linear(expansion * emb_size, emb_size, device=device, dtype=dtype),
        )
    
class TransformerEncoderBlock(nn.Sequential):
    """
    Encoder block

    Structure:
        - Resblock(LayerNorm - MH-Attention - Drop) - Resblock(LayerNorm - Feedforward - Drop)
    """
    def __init__(self,
                 emb_size: int = 768,
                 drop_p: float = 0.,
                 forward_expansion: int = 4,
                 forward_drop_p: float = 0.,
                 input_embed: int = -1,
                 device='cpu',
                 dtype=torch.float32,
                 ** kwargs):
        super().__init__(
            ResidualAdd(nn.Sequential(
                OrderedDict(
                    [
                        ("eb_lrnorm1", nn.LayerNorm(emb_size, device=device, dtype=dtype)),
                        ("eb_mha", MultiHeadAttention(emb_size, input_embed=input_embed, device=device, dtype=dtype, nohidden=False, **kwargs)),
                        ("eb_drop1", nn.Dropout(drop_p)),
                    ]
                )
            )),
            ResidualAdd(nn.Sequential(
                OrderedDict(
                    [
                        ("eb_lrnorm2", nn.LayerNorm(emb_size, device=device, dtype=dtype)),
                        ("eb_ffb", FeedForwardBlock(
                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p, device=device, dtype=dtype),),
                        ("eb_drop2", nn.Dropout(drop_p)),
                    ]
                )
            )
            ))
        
class TransformerEncoder(nn.Sequential):
    def __init__(self, depth: int = 12, input_embed=-1, **kwargs):
        if input_embed < 0: # if not using sepratate input embedding size
            super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])
        else:
            super().__init__(
                TransformerEncoderBlock(input_embed=input_embed, **kwargs),
                *[TransformerEncoderBlock(**kwargs) for _ in range(depth-1)]
            )
                
        
class ClassificationHead(nn.Sequential):
    """
    Classification head attatched to the final encoder output
    (emb size) to n classes
    """
    def __init__(self, emb_size: int = 768, n_classes: int = 1000, device='cpu', dtype=torch.float32):
        super().__init__(
            OrderedDict(
                [
                    ("ch_lrnorm", nn.LayerNorm(emb_size, device=device, dtype=dtype)),
                    ("ch_gap", Reduce('b n e -> b e', reduction='mean')), # average pooling
                    ("ch_linear1", nn.Linear(emb_size, emb_size, device=device, dtype=dtype)),
                    ("ch_tanh", nn.Tanh()),
                    ("ch_linear2", nn.Linear(emb_size, n_classes, device=device, dtype=dtype)),
                ]
            ))



class CustomHead(nn.Module):
    def __init__(self, emb_size: int = 768, n_classes: int = 1000, device='cpu', dtype=torch.float32):
        super(CustomHead, self).__init__()
        
        self.norm = nn.LayerNorm(emb_size, eps=1e-06, elementwise_affine=True, device=device, dtype=dtype)
        self.pre_logits = nn.Identity()
        self.head = nn.Linear(emb_size, n_classes, device=device, dtype=dtype)
        
    def forward(self, x):
        x = self.norm(x)
        x = self.pre_logits(x)
        return self.head(x)

        
class ViT(nn.Module):
    """
    Vision Transformer
    """
    def __init__(self,     
                in_channels: int = 3,
                patch_size: int = 16,
                emb_size: int = 768,
                input_embed: int = -1,
                depth: int = 12,
                n_classes: int = 1000,
                drop_p=0.1,
                pixel_space='RGB',
                ver=1, # model version selector. Has no effect if pixel_space == 'RGB'
                use_subblock=True,
                device='cpu',
                dtype=torch.float32,
                **kwargs):
        if pixel_space.lower()=='rgb' or pixel_space.lower()=='dct2rgb':
            super(ViT, self).__init__()
            self.patchembed = PatchEmbedding(in_channels, patch_size, emb_size, device=device, dtype=dtype)
            self.encoder = TransformerEncoder(depth, emb_size=emb_size, drop_p=drop_p, forward_drop_p=drop_p, device=device, dtype=dtype, **kwargs)
            self.classhead = ClassificationHead(emb_size, n_classes, device=device, dtype=dtype)        
        elif pixel_space.lower()=='dct' or pixel_space.lower()=='rgb2dct':
            super(ViT, self).__init__()
            if ver==1:
                self.patchembed = PatchEmbedding_DCT_Group(patch_size, emb_size, use_subblock, device=device, dtype=dtype)
                self.encoder = TransformerEncoder(depth, emb_size=emb_size, input_embed=input_embed, drop_p=drop_p, forward_drop_p=drop_p, device=device, dtype=dtype, **kwargs)
                self.classhead = ClassificationHead(emb_size, n_classes, device=device, dtype=dtype)
            elif ver==2 and not use_subblock:
                self.patchembed = PatchEmbedding_DCT_Separate(patch_size, emb_size, device=device, dtype=dtype)
                self.encoder = TransformerEncoder(depth, emb_size=emb_size, input_embed=input_embed, drop_p=drop_p, forward_drop_p=drop_p, device=device, dtype=dtype, **kwargs)
                self.classhead = ClassificationHead(emb_size, n_classes, device=device, dtype=dtype)
            elif ver==2 and use_subblock:
                self.patchembed = PatchEmbedding_DCT_Separate_subblock(patch_size, emb_size, device=device, dtype=dtype)
                self.encoder = TransformerEncoder(depth, emb_size=emb_size, input_embed=input_embed, drop_p=drop_p, forward_drop_p=drop_p, device=device, dtype=dtype, **kwargs)
                self.classhead = ClassificationHead(emb_size, n_classes, device=device, dtype=dtype)
            elif ver==3:
                self.patchembed = PatchEmbedding_DCT_Concat(patch_size, emb_size, use_subblock, device=device, dtype=dtype)
                self.encoder = TransformerEncoder(depth, emb_size=emb_size, input_embed=input_embed, drop_p=drop_p, forward_drop_p=drop_p, device=device, dtype=dtype, **kwargs)
                self.classhead = ClassificationHead(emb_size, n_classes, device=device, dtype=dtype)
        self.pixel_space = pixel_space
    def forward(self, x, cbcr=None):
        """
        x: input image or Y channel
        cbcr: optional, cbcr channel
        """
        if self.pixel_space.lower()=='rgb':
            out = self.patchembed(x)
        elif self.pixel_space.lower()=='dct':
            # if x type is tuple, then it's (y, cbcr)
            if isinstance(x, tuple):
                x, cbcr = x
            out = self.patchembed(x, cbcr)
        out = self.encoder(out)
        #out = self.classhead(out)
        return out